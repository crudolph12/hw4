---
title: "HW4"
author: "Akanksha Arora, Chad Rudolph, Sam Stripling"
date: "4/26/2019"
output:
  pdf_document: default
---
### Question 1

```{r include=FALSE}
library(tidyverse)
library(ISLR)
library(mosaic)
library(foreach)
library(cluster)
library(ggalt)
library(ggfortify)

#loading data set
data_file<-'https://raw.githubusercontent.com/jgscott/ECO395M/master/data/wine.csv'
wine<-read.csv(url(data_file))
head(wine)
#take out numerical variables for dimensional reduction and clustering
wine_data = wine[,c(1:11)]


#Rescale and normalize the data
wine_data = scale(wine_data, center = TRUE , scale =TRUE)

#Cluster to red wine and white wine
#Elbow plot to find optimum k
k_grid = seq(2, 20, by=1)
SSE_grid = foreach(k = k_grid, .combine='c') %do% {
  cluster_k = kmeans(wine_data, k, nstart=10)
  cluster_k$tot.withinss
}
```
We begin with clustering.  In this instance, we felt that the elbow plot was the best method for identifying an optimal K.  The elbow plot below identifies K to be a value of 3:

```{r echo=FALSE}
plot(k_grid, SSE_grid)
```
Now that the optimal K is known, we can obtain clusters using this value and seperate red and white wine.  Using the density and pH levels of the wines as references, we observe the breakdown of red vs. white wine.
```{r echo=FALSE}
clust = kmeans(wine_data, 3, nstart=10)
qplot(wine$density,wine$pH, data=wine, shape=factor(clust$cluster), col=factor(wine$color))
```
From this chart, it appears that red wines tend to have higher densities on average compared to white wine, but similar pH levels.  Finally, we observe a table with the breakdown of wine color per cluster:
```{r echo=FALSE}
table = xtabs(~clust$cluster + wine$color)
table
```
White wine seems to dominate the first two clusters, while red wine dwarfs its counterpart in the 3rd cluster.  Next, we move to a PCA analysis.

```{r include=FALSE}
wine$cluster[which(clust$cluster == 3)] = 3

df_wine   <- data.frame(wine, Species=wine$color)
df_wine_1 <- wine[wine$cluster == 1, ]  # df for cluster 1
df_wine_2 <- wine[wine$cluster == 2, ]  # df for cluster 2
df_wine_3 <- wine[wine$cluster == 3, ]  # df for cluster 3
pr_wine = prcomp(wine_data, scale = TRUE)
scores = pr_wine$x
loadings = pr_wine$rotation

#apply PCA to cluster
clustPCA = kmeans(scores[,1:3], 3, nstart=10)
theme_set(theme_classic())
pca_mod <- prcomp(wine_data)  # compute principal components

df_pc <- data.frame(pca_mod$x, Species=wine$cluster, quality=wine$quality, color=wine$color)  # dataframe of principal components
df_pc$Species =factor(df_pc$Species)
df_pc_cluster1 <- df_pc[df_pc$Species == 1,]  # df for cluster 1
df_pc_cluster2 <- df_pc[df_pc$Species == 2,]  # df for cluster 2
df_pc_cluster3 <- df_pc[df_pc$Species == 3,]  # df for cluster 3

theme_set(theme_classic())
pca_mod <- prcomp(wine_data)  # compute principal components

df_pc <- data.frame(pca_mod$x, Species=wine$color)  # dataframe of principal components
df_pc_red <- df_pc[df_pc$Species == "red", ]  # df for 'virginica'
df_pc_white <- df_pc[df_pc$Species == "white", ]  # df for 'setosa'

```

```{r echo=FALSE}
ggplot(df_pc, aes(PC1, PC2, col=Species)) + 
  geom_point(aes(shape=Species), size=2) +   # draw points
  labs(title="Iris Clustering", 
       subtitle="With principal components PC1 and PC2 as X and Y axis",
       caption="Source: Iris") + 
  coord_cartesian(xlim = 1.2 * c(min(df_pc$PC1), max(df_pc$PC1)), 
                  ylim = 1.2 * c(min(df_pc$PC2), max(df_pc$PC2))) +   # change axis limits
  geom_encircle(data = df_pc_red, aes(x=PC1, y=PC2)) +   # draw circles
  geom_encircle(data = df_pc_white, aes(x=PC1, y=PC2))
```
Via higher-order PC analysis we can easily see the difference between red and white wine.  Running PC2 on the residual matrix from PC1 allows for a clearer picture than clustering, clearly separating the two wines.  It is feasible that this method could sort the high quality wine from the low quality, specifically if the qualities were given within a range, such as 0-5 equals low quality and 6-10 equals high quality.
```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(FNN)
library(dplyr)
socialmedia = read.csv('~/Downloads/social_marketing.csv', fill=TRUE ,header = TRUE, stringsAsFactors = FALSE)
dim(socialmedia)
is.data.frame(socialmedia)
attach(socialmedia)

```
###Question 2

NutrientH20 Customer Report:  We have identified several groups that could be a good target for our marketing campaigns.  We outline our process in selecting these groups.  Given that our advertising efforts are expensive, we narrowed down our search to the key group that we should target.  We examined a table of the pairwise correlations between Twitter chat categories.  There was a .81 correlation between health_nutrition and personal_fitness Tweets.  This was the largest correlation observed for any given pair of variables.  Given that we are a nutrition company, this group should be our prime marketing target.  Every individual who had more than three of both health_nutrition and personal_fitness Tweets is considered to be our "Bronze" marketing base.  Our "Silver" marketing base is those individuals with more than 5 of both nutrition and personal fitness Tweets.  Gold is for individuals with over 10 of both.  We list the Gold individuals below.

Note that we considered running K-means clustering to partition the Twitter users into groups.  However, the strong correlation that exists between health_nutrition and personal_fitness Tweets suggests a simpler method of finding our key marketing targets (we are a nutrition company) that can avoid the pitfalls of K-means clustering.  For instance, it is not clear how many clusters to use, nor which distance measure (ex. Euclidean, Taxi Cab) to use in determining our clusters.  We could very easily end up with clusters too broad (that don't allow us to pinpoint our group of interst) or too narrow (and suffer from overfitting).

Gold Targets (Over 10 Tweets in health_nutrition and personal_fitness categories):


```{r correlation, include = FALSE}
summary(socialmedia)
socialmedia1 = subset(socialmedia, select = -c(X))
res <- cor(socialmedia1)
round(res, 2)

Bronze = subset(socialmedia, (health_nutrition > 3) & (personal_fitness > 3), select = c("X"))

Silver = subset(socialmedia, (health_nutrition > 5) & (personal_fitness > 5), select = c("X"))

Gold = subset(socialmedia, (health_nutrition > 10) & (personal_fitness > 10), select = c("X"))

```

```{r}
print(Gold)
```

###Question 3

```{r include=FALSE}
#loading library
library(splitstackshape)
library(tidyverse)
library(arules)
library(arulesViz)

#load data
shopping = read.table(url('https://raw.githubusercontent.com/jgscott/ECO395M/master/data/groceries.txt'),sep ="\t",fill=TRUE ,header = FALSE, stringsAsFactors = FALSE)
#rename v1, bind ID to shopping list
colnames(shopping) <- "bundle"
shopping$shopperID <- as.integer(rownames(shopping))
#match individual goods with each customer
goods = cSplit(shopping, "bundle", sep = ",", direction = "long")
head(goods)
#Now split data based on basket
shopper_bundles = split(x=goods$bundle, f= goods$shopperID)

#test to see if each basket is correct
shopper_bundles[[1]]
shopper_bundles[[10]]
## Remove duplicates ("de-dupe")
# lapply says "apply a function to every element in a list"
shopper_bundles = lapply(shopper_bundles, unique)
head(shopper_bundles)
## Cast this variable as a special arules "transactions" class.
shoptrans = as(shopper_bundles, "transactions")
summary(shoptrans)

# Now run the 'apriori' algorithm
# Look at rules with support > .01 & confidence >.1 & length (# items) <= 10
shoppingrules = apriori(shoptrans, 
                     parameter=list(support=.01, confidence=.1, maxlen=10))


```
After reorganizing the data in a user-friendly way, we began our analysis.  We limited the data to fit the following categories: 1% support, 10% confidence, and a max length of 10.  Below is a graphical summary of the 435 rules this produces.
```{r echo=FALSE}
plot(shoppingrules, measure = c("support", "lift"), shading = "confidence")
```
In the graph above, confidence is displayed by a shade of red, which darkens with higher confidence.  The support is displayed upon the x-axis, and the lift is displayed upon the y-axis.  From the graph, it is easy to tell that the majority of the high confidence, high lift data resides the support range of 1% to 5%.  Specifically, it appears that a lift of 2 yielded many high-confidence rules.  So, we decided to inspect the data at a lift of 2 and a confidence of 50%.  The results are as follows:
```{r echo=FALSE}
arules::inspect(subset(shoppingrules, lift > 2 & confidence > 0.5))
```
In general, it appears whole milk and “other vegetables” go with a variety of bundles.  The items within these bundles make logical sense.  For example, butter and other vegetables match with whole milk.  Interestingly, root vegetables always match with other vegetables at these levels.  We obtain the highest lifts (above 3) when matching fruits and root vegetables with other vegetables.  Again, this should be expected, as many people will buy their fruits and vegetables at the same time.  In conclusion, the most common items that make up a bundle are vegetables and milk, which is unsurprising given their common use in cooking meals.