---
output:
  pdf_document: default
---

# ECO 395M: Exercises 4


### 1: Clustering and PCA

PCA is useful for high-dimensional data but in the context of this dataset. k-means clustering will suffice. The clustering approach is capable of distinguishing between red and white wines as attributes of the wines differ by color. However, it is not able to distinguish between wines of different quality as well.


```{r, include=FALSE}
library(ggplot2)
library(foreach)
library(mosaic)
library(rgl)
library(RColorBrewer)
library(scales)

data_file<-'https://raw.githubusercontent.com/jgscott/ECO395M/master/data/wine.csv'
wine<-read.csv(url(data_file))

summary(wine)

# Center and scale the data
Z = wine[,(1:11)]
Z = scale(Z, center=TRUE, scale=TRUE)

# correlation structure
plot(Z)

#########################
# PCA
#########################

# scaling inside the prcomp function now
pc1 = prcomp(Z, scale.=TRUE)

# Look at the basic plotting and summary methods
summary(pc1)
```

```{r echo=FALSE}
plot(pc1)
```
Above we observe the variance of the pc1 values.
```{r include=FALSE}
# 4 components is both 'elbow' and explains >73% variance

# First for principal components
comp <- data.frame(pc1$x[,1:4])
# Plot
plot(comp, pch=16, col=rgb(0,0,0,0.5))

# Multi 3D plot
plot3d(comp$PC1, comp$PC2, comp$PC3)
plot3d(comp$PC1, comp$PC3, comp$PC4)

loadings = pc1$rotation
scores = pc1$x
```

```{r echo=FALSE}
qplot(scores[,1], scores[,2], color=wine$color, xlab='Component 1', ylab='Component 2')
```
The final results for PCA show a clear split between red and white wine when comparing between component 1 and component 2.
```{r include=FALSE}
#########################
# Clustering
#########################

# Determine number of clusters
set.seed(123)
wss <- (nrow(Z)-1)*sum(apply(Z,2,var))
for (i in 2:11) wss[i] <- sum(kmeans(Z, centers=i)$withinss)
```

```{r echo=FALSE}
plot(1:11, wss, type="b", xlab="Number of Clusters", ylab="Within groups sum of squares")
k <- kmeans(comp, 4, nstart=25, iter.max=1000)

palette(alpha(brewer.pal(9,'Set1'), 0.5))
plot(comp, col=k$clust, pch=16)
boxplot(k$cluster ~ wine$color, xlab='Color', ylab='Cluster', main='Cluster by Wine Color')

# Compare cluster by color in boxplot
boxplot(wine$quality ~ k$cluster, xlab='Cluster', ylab='Quality', main='Wine Quality by Cluster')

```

The final set of graphs show clustering results.  Via the elbow plot test, it was determined k=4.  Using this K, we compared the results of PC1-4 in the colored graph above.  Finally, we utilized a box plot to observe difference in clusters between red and white wine and quality of wine.

### 2: Market segmentation
```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(FNN)
library(dplyr)
socialmedia = read.csv('~/Downloads/social_marketing.csv', fill=TRUE ,header = TRUE, stringsAsFactors = FALSE)
dim(socialmedia)
is.data.frame(socialmedia)
attach(socialmedia)
```

NutrientH20 Customer Report:  We have identified several groups that could be a good target for our marketing campaigns.  We outline our process in selecting these groups.  Given that our advertising efforts are expensive, we narrowed down our search to the key group that we should target.  We examined a table of the pairwise correlations between Twitter chat categories.  There was a .81 correlation between health_nutrition and personal_fitness Tweets.  This was the largest correlation observed for any given pair of variables.  Given that we are a nutrition company, this group should be our prime marketing target.  Every individual who had more than three of both health_nutrition and personal_fitness Tweets is considered to be our "Bronze" marketing base.  Our "Silver" marketing base is those individuals with more than 5 of both nutrition and personal fitness Tweets.  Gold is for individuals with over 10 of both.  We list the Gold individuals below.

Note that we considered running K-means clustering to partition the Twitter users into groups.  However, the strong correlation that exists between health_nutrition and personal_fitness Tweets suggests a simpler method of finding our key marketing targets (we are a nutrition company) that can avoid the pitfalls of K-means clustering.  For instance, it is not clear how many clusters to use, nor which distance measure (ex. Euclidean, Taxi Cab) to use in determining our clusters.  We could very easily end up with clusters too broad (that don't allow us to pinpoint our group of interst) or too narrow (and suffer from overfitting).

Gold Targets (Over 10 Tweets in health_nutrition and personal_fitness categories):


```{r correlation, include = FALSE}
summary(socialmedia)
socialmedia1 = subset(socialmedia, select = -c(X))
res <- cor(socialmedia1)
round(res, 2)

Bronze = subset(socialmedia, (health_nutrition > 3) & (personal_fitness > 3), select = c("X"))

Silver = subset(socialmedia, (health_nutrition > 5) & (personal_fitness > 5), select = c("X"))

Gold = subset(socialmedia, (health_nutrition > 10) & (personal_fitness > 10), select = c("X"))

```

```{r}
print(Gold)
```

###Question 3

```{r include=FALSE}
#loading library
library(splitstackshape)
library(tidyverse)
library(arules)
library(arulesViz)

#load data
shopping = read.table(url('https://raw.githubusercontent.com/jgscott/ECO395M/master/data/groceries.txt'),sep ="\t",fill=TRUE ,header = FALSE, stringsAsFactors = FALSE)
#rename v1, bind ID to shopping list
colnames(shopping) <- "bundle"
shopping$shopperID <- as.integer(rownames(shopping))
#match individual goods with each customer
goods = cSplit(shopping, "bundle", sep = ",", direction = "long")
head(goods)
#Now split data based on basket
shopper_bundles = split(x=goods$bundle, f= goods$shopperID)

#test to see if each basket is correct
shopper_bundles[[1]]
shopper_bundles[[10]]
## Remove duplicates ("de-dupe")
# lapply says "apply a function to every element in a list"
shopper_bundles = lapply(shopper_bundles, unique)
head(shopper_bundles)
## Cast this variable as a special arules "transactions" class.
shoptrans = as(shopper_bundles, "transactions")
summary(shoptrans)

# Now run the 'apriori' algorithm
# Look at rules with support > .01 & confidence >.1 & length (# items) <= 10
shoppingrules = apriori(shoptrans, 
                     parameter=list(support=.01, confidence=.1, maxlen=10))

# Look at the output... so many rules!
inspect(shoppingrules)
```
After reorganizing the data in a user-friendly way, we began our analysis.  We limited the data to fit the following categories: 1% support, 10% confidence, and a max length of 10.  Below is a graphical summary of the 435 rules this produces.
```{r echo=FALSE}
plot(shoppingrules, measure = c("support", "lift"), shading = "confidence")
```
In the graph above, confidence is displayed by a shade of red, which darkens with higher confidence.  The support is displayed upon the x-axis, and the lift is displayed upon the y-axis.  From the graph, it is easy to tell that the majority of the high confidence, high lift data resides the support range of 1% to 5%.  Specifically, it appears that a lift of 2 yielded many high-confidence rules.  So, we decided to inspect the data at a lift of 2 and a confidence of 50%.  The results are as follows:
```{r echo=FALSE}
arules::inspect(subset(shoppingrules, lift > 2 & confidence > 0.5))
```

In general, it appears whole milk and "other vegetables" go with a variety of bundles.  The items within these bundles make logical sense.  For example, butter and other vegetables match with whole milk.  Interestingly, root vegetables always match with other vegetables at these levels.  We obtain the highest lifts (above 3) when matching fruits and root vegetables with other vegetables.  Again, this should be expected as many people will buy their fruits and vegetables at the same time.  In conclusion, the most common items that make up a bundle are vegetables and milk, which is unsurprising given their common use in cooking meals.